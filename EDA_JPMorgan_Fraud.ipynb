{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695203db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0517ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into python \n",
    "df = pd.read_csv('/Users/noimotbakare/Dropbox/Fraud_Payments/data/fraud_payment_data') # storing the data in a pandas DataFrame called df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c189b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4M observation, and 13 features\n",
    "print(df.shape)  # rows, columns\n",
    "print(df.head()) \n",
    "unique_transaction_types = df['Transaction_Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a620c1",
   "metadata": {},
   "source": [
    "Deciding between Sender Id and Sender Account as an identifier \n",
    "\n",
    "Most Sender_IDs map to only one Sender_Account (50,333 IDs).\n",
    "But a significant number (8,738) map to multiple accounts.\n",
    "\n",
    "one-to-many relationship — meaning a single Sender_ID can be associated with multiple Sender_Account values.\n",
    "\n",
    "Each Sender_Account maps to exactly 1 Sender_ID\n",
    "\n",
    "Sender_Account is the more granular identifier, it uniquely identifies an account.\n",
    "Sender_Id is a higher-level entity identifier, it identifies the customer, who may own many accounts.\n",
    "\n",
    "For our analysis we will use Sender and Bene Accounts as unique Identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a91f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both are non-null in same rows\n",
    "both_present = df[['Sender_Id', 'Sender_Account']].notnull().all(axis=1).mean()\n",
    "print(f\"Percentage of rows where BOTH Sender_ID and Sender_Account are present: {both_present * 100:.2f}%\")\n",
    "#Does each sender ID map to a Sender Account \n",
    "id_to_account = df.groupby('Sender_Id')['Sender_Account'].nunique().value_counts()\n",
    "print(\"Unique Sender_Account counts per Sender_ID:\")\n",
    "print(id_to_account.head())\n",
    "\n",
    "\n",
    "account_to_id = df.groupby('Sender_Account')['Sender_Id'].nunique().value_counts()\n",
    "print(\"Unique Sender_ID counts per Sender_Account:\")\n",
    "print(account_to_id.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d46ef3",
   "metadata": {},
   "source": [
    "Investigating column Sender Sector \n",
    "maybe industry classification\n",
    "\n",
    "These fraud rates by sector for example 21.4%, 21.0% are 10x the base fraud rate of ~2%. That means some sectors are highly concentrated with fraud.\n",
    "This suggests Sender_Sector is informative and should be kept. \n",
    "I also found that it is categorical in nature.\n",
    "Has many unique values (high cardinality) and shows clear fraud signal (target separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcc235",
   "metadata": {},
   "source": [
    "Because Sender_Sector is a categorical feature represented with numbers, not a true continuous numeric variable. NaN likely doesn't mean that it's randomly missing. It likely indicates a specific category such as unknown sector, unclassified customer, or some sort of corporate default. We want the model to use the missing info. I use \"-1\" to create a clear label for missing/unknown category instead of letting the model guess. \n",
    "\n",
    "\n",
    "\n",
    "Creates a distinct category - Allows the model to learn if missing sector data is associated with higher or lower fraud risk\n",
    "Prevents errors in graph/network or encoding steps.\n",
    "Keeps the value outside real sector codes\t-1 is clearly not a valid sector ID and won’t be confused with real values.\n",
    "Often missing sector itself is a predictive feature\tIn fraud data, missing info is frequently a red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d098543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['Sender_Sector'].value_counts(dropna=False).head(20)) \n",
    "fraud_rate_by_sector = df.groupby('Sender_Sector')['Label'].mean().sort_values(ascending=False)\n",
    "#print(fraud_rate_by_sector.head(50))\n",
    "#print(fraud_rate_by_sector.tail(50))\n",
    "#All the cases where fraud rate by sender sector >0\n",
    "print(fraud_rate_by_sector.loc[fraud_rate_by_sector['Label']>0]) \n",
    "# #Checking for correlation \n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# import numpy as np\n",
    "\n",
    "# # Drop NaN for mutual info calculation\n",
    "# subset = df[['Sender_Sector', 'Label']].dropna()\n",
    "# mutual_info = mutual_info_classif(subset[['Sender_Sector']], subset['Label'], discrete_features='auto')\n",
    "# mutual_info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99741e8",
   "metadata": {},
   "source": [
    "I will calculate average fraud rate for each Sender_Sector, the fraud likelihood by sector (on the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57184145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual calculation of sender sector to compare to transform\n",
    "df['Sender_Sector'] = df['Sender_Sector'].fillna(-1)\n",
    "sector_target_map = df.groupby('Sender_Sector')['Label'].mean()\n",
    "df['Sender_Sector_target_enc'] = df['Sender_Sector'].map(sector_target_map)\n",
    "global_fraud_rate = df['Label'].mean()\n",
    "df['Sender_Sector_target_enc'].fillna(global_fraud_rate, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65024172",
   "metadata": {},
   "source": [
    "Sender Lob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency \n",
    "df['Sender_lob'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Sender_lob')['Label'].mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Sender_lob').agg(\n",
    "    count=('Label', 'count'),\n",
    "    fraud_rate=('Label', 'mean')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Sender Lob against sender sector_target_map\n",
    "pd.crosstab(df['Sender_lob'], df['Sender_Sector'].isna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d474129",
   "metadata": {},
   "source": [
    "Benford's Law predicts that in many real-world datasets, the first digit of a transaction is not random, but follows a logarithmic distribution ( distributed exponentially) where '1' is the most frequent (30.1%), followed by '2' (17.6%), and so on, with '9' being the least frequent (4.6%). Conversely, the last digit of a transaction is expected to be uniformly distributed, meaning each digit (0 to 9)  has roughly an equal chance of appearing, though this is a less common application of the law.\n",
    " \n",
    "\n",
    "Having a sudden jump in popularity of some particular first digits, like 9 or 5, could suggest fraud, maybe to avoid reporting threshold because if you make a transaction like 9,999 it will not be above the reporting threshold of say 10,000. All transaction in US banks that are 10,000 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Calculating  Benford's Law in the data \n",
    "# ----------------------------------------------------------------------\n",
    "df['USD_amount'] = pd.to_numeric(df['USD_amount'], errors='coerce')\n",
    "#Filter out values less than 1\n",
    "#df = df[df['USD_amount'] >= 1]\n",
    "\n",
    "#Extract the first digit from each amount\n",
    "df['first_digit'] = df['USD_amount'].astype(str).str.strip().str[0]\n",
    "df = df[df['first_digit'].str.isdigit()]\n",
    "df['first_digit'] = df['first_digit'].astype(int)\n",
    "\n",
    "\n",
    "#Count how many times each first digit appears per group\n",
    "digit_counts = (\n",
    "    df.groupby(['first_digit', 'Label'])\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "#Compute percentage distribution by label\n",
    "digit_counts['percentage'] = (\n",
    "    digit_counts.groupby('Label')['count']\n",
    "    .apply(lambda x: x / x.sum() * 100)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "plot_df = digit_counts.pivot(index='first_digit', columns='Label', values='percentage')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Add Benford's Law reference\n",
    "# ----------------------------------------------------------------------\n",
    "# Calculate Benford's Law distribution\n",
    "digits = np.arange(1, 10)\n",
    "benford_prop = np.log10(1 + 1 / digits) * 100\n",
    "benford_df = pd.DataFrame({'benford_percentage': benford_prop}, index=digits)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot the observed distributions as bars\n",
    "plot_df.plot(kind='bar', ax=ax, width=0.8, align='center', alpha=0.7)\n",
    "\n",
    "# Plot the Benford's Law distribution as a line\n",
    "ax.plot(benford_df.index, benford_df['benford_percentage'], \n",
    "        marker='o', color='red', linestyle='--', linewidth=2, label='Benford\\'s Law')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# plot formatting\n",
    "# ----------------------------------------------------------------------\n",
    "ax.set_xlabel('First Digit of USD Amount')\n",
    "ax.set_ylabel('Percentage of Transactions (%)')\n",
    "ax.set_title('Benford\\'s Law Analysis by % of Transaction')\n",
    "ax.set_xticklabels(plot_df.index.astype(int), rotation=0)\n",
    "ax.legend(title='Fraud Label')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['last_digit__rounded'] = df['USD_amount'].round(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc4758",
   "metadata": {},
   "source": [
    "Most Common approch - The Last Digit of the Cents (the second decimal place, e.g., the '4' in $12.34).Should be ≈10% for each digit (0-9).Fraudsters sometimes avoid or favor specific cents values (like .00 or .99), distorting this uniform expectation.\n",
    "\n",
    "Less Common, the Digit Before the Decimal (the units place, e.g., the '2' in $12.34).Should also be ≈10%for each digit (0-9).This is less commonly tested on its own because the digits before the decimal are already constrained by Benford's Law, which predicts a non-uniform, logarithmic distribution for those positions.\n",
    "\n",
    "\n",
    "I don't think it makes sense to do this for bene because its the same amount that's going from sender to beneficiary, therefore it maybe redundant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ddc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the last digit\n",
    "#last digit before decimal \n",
    "\n",
    "\n",
    "\n",
    "#Lastdigit before the decimal \n",
    "#df['last_digit_before_dec'] = df['USD_amount'].astype(str).str.split('.').str[0].str[-1]\n",
    "#Extract the last digit after decimal USD_amount\n",
    "df['last_digit_after_dec'] = df['USD_amount'].astype(str).str.split('.').str[1].str[-1]\n",
    "#Keep only rows where the last character is a digit\n",
    "df = df[df['last_digit_after_dec'].str.isdigit()]\n",
    "df['last_digit_after_dec'] = df['last_digit_after_dec'].astype(int)\n",
    "\n",
    "# Filter \n",
    "#df = df[df['USD_amount'] >= 1]\n",
    "\n",
    "#Count how often each last digit appears per fraud label\n",
    "# Group by last digit and Label (1 = fraud, 0 = not fraud)\n",
    "digit_counts1 = (\n",
    "    df.groupby(['last_digit_after_dec', 'Label'])\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "#Calculate percentages within each label\n",
    "# Compute percentages within each Label group\n",
    "digit_counts1['percentage'] = (\n",
    "    digit_counts1.groupby('Label')['count']\n",
    "    .apply(lambda x: x / x.sum() * 100)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "plot_df2 = digit_counts1.pivot(index='last_digit_after_dec', columns='Label', values='percentage')\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plot_df2.plot(kind='bar', ax=ax, width=0.8, align='center', alpha=0.7)\n",
    "\n",
    "# Add a uniform distribution reference line\n",
    "ax.axhline(y=10, color='red', linestyle='--', linewidth=2, label='Expected Uniform (10%)')\n",
    "\n",
    "ax.set_xlabel('Last Digit of USD Amount (after decimal)')\n",
    "ax.set_ylabel('Percentage of Transactions (%)')\n",
    "ax.set_title('Last Digit Distribution by Transaction Type')\n",
    "ax.set_xticklabels(plot_df2.index.astype(int), rotation=0)\n",
    "ax.legend(title='Transaction Type')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19413ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the last digit\n",
    "#last digit before decimal \n",
    "\n",
    "\n",
    "\n",
    "#Lastdigit before the decimal \n",
    "#df['last_digit_before_dec'] = df['USD_amount'].astype(str).str.split('.').str[0].str[-1]\n",
    "#Extract the last digit after decimal USD_amount\n",
    "#df['last_digit_after_dec'] = df['USD_amount'].astype(str).str.split('.').str[1].str[-1]\n",
    "#Keep only rows where the last character is a digit\n",
    "df3 = df[df['last_digit_before_dec'].str.isdigit()]\n",
    "df3['last_digit_before_dec'] = df3['last_digit_before_dec'].astype(int)\n",
    "\n",
    "# Filter \n",
    "#df = df[df['USD_amount'] >= 1]\n",
    "\n",
    "#Count how often each last digit appears per fraud label\n",
    "# Group by last digit and Label (1 = fraud, 0 = not fraud)\n",
    "digit_counts3 = (\n",
    "    df3.groupby(['last_digit_before_dec', 'Label'])\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "#Calculate percentages within each label\n",
    "# Compute percentages within each Label group\n",
    "digit_counts3['percentage'] = (\n",
    "    digit_counts3.groupby('Label')['count']\n",
    "    .apply(lambda x: x / x.sum() * 100)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "plot_df3 = digit_counts3.pivot(index='last_digit_before_dec', columns='Label', values='percentage')\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plot_df3.plot(kind='bar', ax=ax, width=0.8, align='center', alpha=0.7)\n",
    "\n",
    "# Add a uniform distribution reference line\n",
    "ax.axhline(y=10, color='red', linestyle='--', linewidth=2, label='Expected Uniform (10%)')\n",
    "\n",
    "ax.set_xlabel('Last Digit of USD Amount (before decimal)')\n",
    "ax.set_ylabel('Percentage of Transactions (%)')\n",
    "ax.set_title('Last Digit Distribution by Transaction Type')\n",
    "ax.set_xticklabels(plot_df3.index.astype(int), rotation=0)\n",
    "ax.legend(title='Transaction Type')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67191d4",
   "metadata": {},
   "source": [
    "\n",
    "Sender_mean_quick_payments -Follows a U shaped pattern. Starting from bin 0 to bin 14, the fraud rate decreases at first, reaches a minimum around bins 6–8, then increases significantly toward the highest bin (14). The highest fraud percentage (25.6%) occurs in the top quantile. As the mean feature increases into the highest range, fraud likelihood increases sharply. The trend is not perfectly monotonic, the extremes show strong separation: low bins have around ~19–20% fraud, but the highest bin jumps to ~26%. Feature may have predictive power.\n",
    "\n",
    "\n",
    "Sender_std_quick_payments- Fraud rate drops sharply from bin 0 to bin 5 and then spikes again in bin 7–8.s The strong separation pattern, tells us that variability in transaction amounts is predictive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare relative risk of quick payments across users normalized way (like the example you're replicating)\n",
    "quick_payments_df = df[df['Transaction_Type'] == 'QUICK-PAYMENT'].copy()\n",
    "\n",
    "# Perform the groupby and aggregation correctly\n",
    "# Group by both Sender_Id and Label to get separate stats for fraudulent and non-fraudulent payments\n",
    "sender_quick_stats = quick_payments_df.groupby(['Sender_Account', 'Label']).agg(\n",
    "    count=('USD_amount', 'count'),\n",
    "    mean=('USD_amount', 'mean'),\n",
    "    std=('USD_amount', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Now the `std` column exists and can be accessed\n",
    "# Handle cases where a user has only one transaction, which results in a NaN for std\n",
    "sender_quick_stats['std'] = sender_quick_stats['std'].fillna(0)\n",
    "\n",
    "#print(sender_quick_stats.head())\n",
    "\n",
    "bins = 15\n",
    "sender_quick_stats['count_bin'] = pd.qcut(sender_quick_stats['count'], bins, labels=False, duplicates='drop')\n",
    "sender_quick_stats['mean_bin'] = pd.qcut(sender_quick_stats['mean'], bins, labels=False, duplicates='drop')\n",
    "sender_quick_stats['std_bin'] = pd.qcut(sender_quick_stats['std'], bins, labels=False, duplicates='drop')\n",
    "\n",
    "\n",
    "sender_quick_stats['mean_bin'] = pd.qcut(sender_quick_stats['mean'], bins, labels=False, duplicates='drop')\n",
    "# Calculate % of fraud per bin\n",
    "mean_fraud_dist = sender_quick_stats.groupby(['mean_bin', 'Label']).size()\n",
    "mean_fraud_percent = mean_fraud_dist.groupby(level=0).apply(lambda x: x / x.sum()).unstack(fill_value=0)\n",
    "\n",
    "# # Plot\n",
    "mean_fraud_percent.plot(kind='bar', figsize=(14, 6))\n",
    "plt.title('Sender Mean Quick Pay by is fraud - % of transactions')\n",
    "plt.xlabel('Sender Mean Quick Pay')\n",
    "plt.ylabel('% of transactions')\n",
    "plt.legend(title='is fraud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94804d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % of fraud per bin\n",
    "std_fraud_dist = sender_quick_stats.groupby(['std_bin', 'Label']).size()\n",
    "std_fraud_percent = std_fraud_dist.groupby(level=0).apply(lambda x: x / x.sum()).unstack(fill_value=0)\n",
    "\n",
    "# # Plot\n",
    "std_fraud_percent.plot(kind='bar', figsize=(14, 6))\n",
    "plt.title('Sender STD Quick Pay by is fraud - % of transactions')\n",
    "plt.xlabel('Sender STD Quick Pay')\n",
    "plt.ylabel('% of transactions')\n",
    "plt.legend(title='is fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d98e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total transaction count for each Sender_Country\n",
    "# Sender_transactions_by_country = df['Sender_Country'].value_counts()\n",
    "# #Bene_transactions_by_country = df['Bene_Country'].value_counts()\n",
    "# # Print the result\n",
    "# print(Sender_transactions_by_country)\n",
    "# # Print the result\n",
    "# #print(Bene_transactions_by_country)\n",
    "# # Calculate the total count of fraud transactions (Label=1) for each Sender_Country\n",
    "# fraud_transactions_by_country = (\n",
    "#     df.groupby('Sender_Country')['Label']\n",
    "#     .sum()  # Sums the '1's (fraudulent transactions) within each country group\n",
    "#     .sort_values(ascending=False)\n",
    "# )\n",
    "# print(fraud_transactions_by_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2c529",
   "metadata": {},
   "source": [
    "Fraud rate by country \n",
    "The Financial Action Task Force (FATF) leads global action to tackle money laundering, terrorist and proliferation financing.\n",
    "FATF suggest that some countries have higher fraud risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279886f",
   "metadata": {},
   "source": [
    "\n",
    "I visualize top 50 offenders fraud rate by country. \n",
    "\n",
    "High-Volume Countries - countries that have a sufficient number of transactions, making their calculated rate statistically reliable. They will use their specific calculated country fraud rate.\n",
    "\n",
    "Low-Volume Countries - that have  small sample data, but the small sample size makes their calculated rate noisy or statistically unreliable. Use global average is used to smooth this rate toward the mean ( Bayesian averaging), making it more stable and reliable.\n",
    "\n",
    "Unseen/New Countries - countries that have no historical data on which to base a rate, so the calculated country-specific feature is NaN. The global average serves as the most logical, unbiased estimate for their rate.\n",
    "\n",
    "The gloabl fraud rate is - 2.06\n",
    "\n",
    "\n",
    "(if agreed on I can add to main df and use global average for countries wiht no historical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5266259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Factorize Sender_Country into numeric codes\n",
    "#Sender_Country_df['sender_country_code'] = pd.factorize(Sender_Country_df['Sender_Country'])[0]\n",
    "\n",
    "\n",
    "#Calculate Total Transactions per country\n",
    "total_transactions = df['Sender_Country'].value_counts().rename('Total Transactions per country')\n",
    "\n",
    "#Calculate Fraudulent Transactions per country\n",
    "fraud_transactions = df.groupby('Sender_Country')['Label'].sum().rename('Fraudulent Transactions per country')\n",
    "\n",
    "#Combine the two Series into a single DataFrame\n",
    "results_df = pd.concat([total_transactions, fraud_transactions], axis=1)\n",
    "\n",
    "# 4. Final formatting and Fraud Rate calculation\n",
    "results_df = results_df.reset_index().rename(columns={'index': 'Sender_Country'})\n",
    "results_df['Fraud Rate (%)'] = (results_df['Fraudulent Transactions per country'] / results_df['Total Transactions per country']) * 100\n",
    "\n",
    "# Sort by Fraud Rate (%) and get top 50 offenders\n",
    "top_50_offenders = results_df.sort_values(by='Fraud Rate (%)', ascending=False).head(50)\n",
    "\n",
    "# Display the result\n",
    "top_50_offenders\n",
    "\n",
    "# 1. Prepare the data for plotting\n",
    "countries = top_50_offenders['Sender_Country']\n",
    "fraud_rates = top_50_offenders['Fraud Rate (%)']\n",
    "\n",
    "# 2. Create the figure and axes\n",
    "# Use a large figure size for 50 bars to ensure readability\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 3. Create the bar chart\n",
    "plt.bar(countries, fraud_rates)\n",
    "\n",
    "# 4. Set labels and title\n",
    "plt.xlabel('Sender Country', fontsize=12)\n",
    "plt.ylabel('Fraud Rate (%)', fontsize=12)\n",
    "plt.title('Top 50 Countries by Fraud Rate', fontsize=14)\n",
    "\n",
    "# 5. Rotate the X-axis labels for better visibility\n",
    "plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "\n",
    "# 6. Adjust layout to prevent labels from being cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# 7. Save the plot\n",
    "plt.savefig('top_50_fraud_rate_by_country.png')\n",
    "\n",
    "print(\"Plot saved as top_50_fraud_rate_by_country.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation with the global mean\n",
    "\n",
    "# Calculate the overall global fraud rate\n",
    "global_total_fraud = df['Label'].sum() \n",
    "global_total_tx = len(df)\n",
    "global_avg_rate = (global_total_fraud / global_total_tx) * 100\n",
    "print(global_avg_rate)\n",
    "# Fill NaN values (new/unseen countries) with the global average\n",
    "#df['Country_Fraud_Rate_Feature'] = df['Country_Fraud_Rate_Feature'].fillna(global_avg_rate)\n",
    "\n",
    "# For better smoothing consider using a weighted average technique, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bdcfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73974f34",
   "metadata": {},
   "source": [
    "Time related variables \n",
    "\n",
    "Everything below this line is not concrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dtttm format\n",
    "df['Time_step']= pd.to_datetime(df['Time_step'])\n",
    "\n",
    "#Sort by sender and time to correctly calculate time based features \n",
    "df=df.sort_values(by=['Sender_Account', 'Time_step'])\n",
    "# ===========\n",
    "#mean time between transactioin - this tells us whether users sends transactions frequently or infrequently ( good for detecting usual behavior vs. sudden shifts in behavior)\n",
    "# ==============\n",
    "#Time between transactions \n",
    "df['time_diff']= df.groupby('Sender_Account')['Time_step'].diff()\n",
    "df['time_diff_seconds']= df['time_diff'].dt.total_seconds()\n",
    "# df['time_diff_minutes']= df['time_diff'].dt.total_minutes()\n",
    "# df['time_diff_hours']= df['time_diff'].dt.total_hours()\n",
    "# Aggregate to compute mean time between transactions per sender\n",
    "mean_time_df = df.groupby('Sender_Account')['time_diff_seconds'].mean().reset_index()\n",
    "mean_time_df.rename(columns={'time_diff_seconds': 'mean_time_between_transactions'}, inplace=True)\n",
    "\n",
    "#filling NAN with median \n",
    "median_time = mean_time_df['mean_time_between_transactions'].median()\n",
    "mean_time_df['mean_time_between_transactions'].fillna(median_time, inplace=True)\n",
    "\n",
    "mean_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=======\n",
    "#Transaction Velocity \n",
    "#=======\n",
    "#sender velocity on the other hand tells us the number of transactions over a moving time window. Velocity = Number of transactions in a window/ window duration \n",
    "#High velocity can signal money laundering - bursty or rapid behavior. It's more about transaction intensity \n",
    "# Function to compute rolling count for each sender\n",
    "def compute_velocity(group, window='24H'):\n",
    "    return (\n",
    "        group\n",
    "        .set_index('Time_step')  # use Time_Step as index temporarily\n",
    "        .rolling(window=window)['Transaction_Id']\n",
    "        .count()\n",
    "        .values  # get array aligned to original group index\n",
    "    )\n",
    "\n",
    "# Apply function group-wise using transform-like behavior\n",
    "df['velocity_last_24h'] = df.groupby('Sender_Account', group_keys=False).apply(\n",
    "    lambda g: compute_velocity(g, window='24H')\n",
    ")\n",
    "\n",
    "# Fill any NaN (usually first transactions) with 0\n",
    "df['velocity_last_24h'] = df['velocity_last_24h'].fillna(0)\n",
    "\n",
    "print(df[['Sender_Account', 'Time_step', 'velocity_last_24h']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Sender_Account', 'Time_step', 'velocity_last_24h']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85427e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Mean time between txn\n",
    "df = df.merge(mean_time_df, on='Sender_Account', how='left')\n",
    "df['mean_time_between_transactions'] = df['mean_time_between_transactions']\n",
    "# # Merge Velocity in last 24 hours to df\n",
    "# df = df.merge(mean_time_df, on='Sender_Account', how='left')\n",
    "# df['velocity_last_24h'] = df['velocity_last_24h'].fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c590c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Boxplot for mean time between transactions\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(x='Label', y='mean_time_between_transactions', data=df)\n",
    "plt.title('Mean Time Between Transactions vs Fraud Label')\n",
    "plt.xticks([0,1], ['Non-Fraud', 'Fraud'])\n",
    "\n",
    "# Boxplot for velocity\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x='Label', y='velocity_last_24h', data=df)\n",
    "plt.title('Velocity (Last 24h) vs Fraud Label')\n",
    "plt.xticks([0,1], ['Non-Fraud', 'Fraud'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(mean_time_between_transactions_x,mean_time_between_transactions_y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3feefe1",
   "metadata": {},
   "source": [
    "Recalcualte this to be a colum nonfraud transaction rate =  non fraud transaction/ Total Transaction  and Fraud Transaction rate=  Fraud Transaction/ Total Transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk if this is RIGHT _ Including Fraud label is throwing off the calculation and this makes sense mathematically \n",
    "# Filter out rows with missing Sender_Country\n",
    "Sender_Country_df = df[df['Sender_Country'].notna()].copy()\n",
    "\n",
    "# Factorize Sender_Country into numeric codes\n",
    "Sender_Country_df['sender_country_code'] = pd.factorize(Sender_Country_df['Sender_Country'])[0]\n",
    "\n",
    "# Group by both Sender_Country and Label (0 = non-fraud, 1 = fraud)\n",
    "sender_country_fraud_stats = Sender_Country_df.groupby(['Sender_Country','sender_country_code','Label']).agg(\n",
    "    total_tx=('Sender_Account', 'count'),   # total transactions per group\n",
    "    fraud_tx=('Label', 'count')      # fraud count (Label=1)\n",
    ").reset_index()\n",
    "\n",
    "sender_country_fraud_stats['fraud_rate'] = sender_country_fraud_stats['fraud_tx'] / sender_country_fraud_stats['total_tx']\n",
    "sender_country_fraud_stats\n",
    "sender_country_fraud_stats['non_fraud_rate'] = sender_country_fraud_stats['fraud_tx'] / sender_country_fraud_stats['total_tx']\n",
    "sender_country_fraud_stats\n",
    "\n",
    "# Sort by Fraud Rate (%) and get top 50 offenders\n",
    "top_50_offenders1 = sender_country_fraud_stats.sort_values(by='fraud_rate', ascending=False).head(50)\n",
    "\n",
    "# Display the result\n",
    "top_50_offenders1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
