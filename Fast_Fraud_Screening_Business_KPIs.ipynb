{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss,average_precision_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "#used for calling models from other notebooks\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_features=pd.read_csv('total_features', sep=',', header=0) \n",
    "#final_features=pd.read_csv('total_features', sep=',', header=0) \n",
    "final_features = pd.read_csv('total_features', sep=',', header=0)\n",
    "total_data = pd.read_csv('fraud_payment_data', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a31be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=total_data[total_data.USD_amount>0]\n",
    "total_data=total_data.reset_index(drop=True)\n",
    "total_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming the length of both tables before merging\n",
    "len(final_features), len(total_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check to verify the two dfs are align\n",
    "(final_features.index == total_data.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding USD to final features because they were not included in the modeling features \n",
    "final_features[\"USD_amount\"] = total_data[\"USD_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9424c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split for train, validate and test data\n",
    "train_features=final_features[0:1000000]\n",
    "validate_features=final_features[1000000:1250000]\n",
    "test_features=final_features[1250000:-1]\n",
    "\n",
    "y_train=total_data['Label'][0:1000000]\n",
    "y_validate=total_data['Label'][1000000:1250000]\n",
    "y_test=total_data['Label'][1250000:-1]\n",
    "\n",
    "##Scaler for eventual Logistic regression\n",
    "X=StandardScaler().fit_transform(train_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running XGboost to identify threshold  \n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Train model\n",
    "xgb_model= XGBClassifier(\n",
    "    max_depth=70,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=700,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=10  # fraud imbalance handling\n",
    ")\n",
    "\n",
    "xgb_model.fit(train_features, y_train)\n",
    "\n",
    "# # Use the trained logistic regression model to predict how likely each transaction in X_test is to be fraudulent.\n",
    "# y_test_xgb_pr = xgb_model.predict_proba(X_test)[:,1]\n",
    "# Predict probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32853937",
   "metadata": {},
   "source": [
    "                                               Business KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame from X_test\n",
    "df_test = pd.DataFrame(test_features)\n",
    "\n",
    "# Add the true fraud labels\n",
    "df_test[\"is_fraud\"] = y_test.values\n",
    "\n",
    "# Add the model's continuous score (probability between 0 and 1)\n",
    "df_test[\"model_score\"] = xgb_model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "#sanity check column for upcoming calculations \n",
    "fraud_df =df_test[df_test[\"is_fraud\"] == 1]\n",
    "\n",
    "fraud_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a250959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelscore - it's a continous scrore from the model that gives a 0-1 probability \n",
    "df_test[\"model_score\"] = xgb_model.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f38ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model flag \n",
    "# Threshold to create model_flag\n",
    "threshold = 0.5 #the ideal threshold determine in our analysis \n",
    "df_test[\"model_flag\"] = (df_test[\"model_score\"] >= threshold).astype(int)\n",
    "\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#√Synthetic loss or fraud Loss Avoided score\n",
    "#missed fraud risk score\n",
    "#√loss avoided accumulates only when we catch fraud\n",
    "#\n",
    "df_test[\"synthetic_loss\"] = df_test[\"USD_amount\"] * df_test[\"is_fraud\"]\n",
    "df_test[\"missed_risk\"] = df_test[\"synthetic_loss\"] * (1- df_test[\"model_flag\"])\n",
    "df_test[\"loss_avoided\"] = df_test[\"synthetic_loss\"] * df_test[\"model_flag\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ea7ae",
   "metadata": {},
   "source": [
    "                                               Synthetic Loss, Missed Risk and Loss Avoided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = df_test[\"synthetic_loss\"].sum()\n",
    "missed_risk = df_test[\"missed_risk\"].sum()\n",
    "loss_avoided = df_test[\"loss_avoided\"].sum()\n",
    "\n",
    "missed_risk_pct = missed_risk / total_loss\n",
    "loss_avoided_pct = loss_avoided / total_loss\n",
    "\n",
    "\n",
    "print(\"total loss $:\", total_loss,\n",
    "       \"loss avoided\", loss_avoided,\n",
    "      \"missed risk\", missed_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualizing model performace on Fraud Losses\n",
    "plt.bar([\"Loss avoided\", \"Missed risk\"], [loss_avoided, missed_risk], color=[\"#42A2B9\", \"crimson\"])\n",
    "plt.title(\"Model Performance on Fraud Losses\")\n",
    "plt.ylabel(\"USD (Millions)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bd7ee",
   "metadata": {},
   "source": [
    "## Alert Cost Simulation – The Cost of Analysts Reviewing Alerts\n",
    "\n",
    "Realistically, the estimated cost per analyst ranges from:  \n",
    "$1–$5 for FinTech firms  \n",
    "$3–$12 for mid-size banks  \n",
    "$20–$70 for larger banks  \n",
    "\n",
    "Since our data is synthetic data from **JPMorgan Chase**, we use the **average cost** from larger banks.  \n",
    "\n",
    "**Assumption:**  \n",
    "The average analyst at JPMorgan Chase earns **$50/hour**.  \n",
    "Given approximately **12 minutes** to review the average case,  \n",
    "we estimate a **Synthetic Average Review Cost** of:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\$10 = \\left(\\frac{\\$50}{60\\ \\text{minutes}}\\right) \\times 12\\ \\text{minutes}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_Avg_review_cost = (50/60) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review cost \n",
    "df_test[\"review_cost\"] = df_test[\"model_flag\"] * synthetic_Avg_review_cost\n",
    "total_review_cost = df_test[\"review_cost\"].sum()\n",
    "print(\"Total review cost $ :\", total_review_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e69b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total loss avoided (dollars)\n",
    "total_loss_avoided = df_test.loc[df_test[\"loss_avoided\"] > 0, \"loss_avoided\"].sum()\n",
    "\n",
    "print(\"Total loss avoided in $:\", total_loss_avoided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of positive loss avoided \n",
    "fraud_caught = (df_test[\"loss_avoided\"] > 0).sum()\n",
    "fraud_caught"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74049a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to count of fraud \n",
    "total_fraud = (df_test[\"is_fraud\"] == 1).sum()\n",
    "total_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate of fraud caught \n",
    "fraud_catch_rate = fraud_caught/total_fraud\n",
    "fraud_catch_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21557fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#false positive rate \n",
    "#% of legit transactions incorrectly flagged as fraud\n",
    "fpr = df_test[(df_test.is_fraud==0) & (df_test.model_flag==1)].shape[0] / df_test[df_test.is_fraud==0].shape[0]\n",
    "\n",
    "#FNR = percentage of fraudulent transactions that model missed\n",
    "fnr = df_test[(df_test.is_fraud==1) & (df_test.model_flag==0)].shape[0] / df_test[df_test.is_fraud==1].shape[0]\n",
    "\n",
    "print(\"False Positive Rate is:\", fpr)\n",
    "print(\"False Negative Rate is:\", fnr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
