{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import networkx as nx #Requires 'pip install networkx'\n",
    "import community as community_louvain  # Requires 'pip install python-louvain'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fraud_payment_data', sep=',', header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting time into a Datetime object\n",
    "df['timestamp'] = pd.to_datetime(df['Time_step'])\n",
    "#Removing redudant columns\n",
    "df=df.drop('Time_step',axis=1)\n",
    "df=df.drop('Sender_lob',axis=1)\n",
    "#df=df.drop('Sender_Sector',axis=1)\n",
    "#Apparently some transactions amounted to zero dollars. None of them were fraudulent, so I've removed them.\n",
    "df=df[df.USD_amount>0]\n",
    "df=df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sender_Sector']=df['Sender_Sector'].fillna(-1)\n",
    "df['Sender_Account']=df['Sender_Account'].fillna(df['Bene_Account'])\n",
    "df['Bene_Account']=df['Bene_Account'].fillna(df['Sender_Account'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take about 70% of our dataset as training data, about 15% as a validation set and use the remaining 15% as our test set.\n",
    "cutoff = round(0.7*len(df)) \n",
    "df_train = df.head(cutoff)\n",
    "not_train =  df.tail(len(df)-cutoff)\n",
    "cutoff2 = round(0.5*len(not_train))\n",
    "df_val = not_train.head(cutoff2)\n",
    "df_test = not_train.tail(len(not_train)-cutoff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_feature_engineerer(graph, dataframe):\n",
    "   \n",
    "    # augments a data frame with some engineered features\n",
    "\n",
    "    # First we generate dataframes to store all the info.\n",
    "    # Then we merge all of the dataframes together into one.\n",
    "    # Then we merge the original dataframe with our dataframe consisting of engineered features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## ---- First we'll create a 'proximity_to_fraud' feature and integrate it into our original dataframe ------\n",
    "\n",
    "    ## Identify fraudulent nodes\n",
    "    fraudulent_nodes=set(dataframe.loc[dataframe['Label']==1]['Sender_Account']).union(set(dataframe.loc[dataframe['Label']==1]['Bene_Account']))\n",
    "\n",
    "\n",
    "    # I asked Google to help write the following chunk of code... We can change it later.\n",
    "\n",
    "    distance_to_fraud = {node: -1 for node in graph.nodes()}\n",
    "\n",
    "    # For each node, find the shortest path length to *any* fraudulent node\n",
    "    for source_node in graph.nodes():\n",
    "        try:\n",
    "            path_lengths = nx.single_source_shortest_path_length(graph, source=source_node, cutoff=5)\n",
    "            min_distance = float('inf')\n",
    "            for target_node in fraudulent_nodes:\n",
    "                if target_node in path_lengths:\n",
    "                    min_distance = min(min_distance, path_lengths[target_node])\n",
    "            if min_distance != float('inf'):\n",
    "                distance_to_fraud[source_node] = min_distance\n",
    "        except nx.NetworkXNoPath:\n",
    "            # If a node is not connected to the main component, the distance is infinite\n",
    "            pass\n",
    "\n",
    "    # Assign distance as an edge attribute ---\n",
    "    # For each edge (transaction), assign a feature representing its proximity to fraud.\n",
    "    # The proximity is defined as the minimum distance of its two endpoints to a fraudulent node.\n",
    "\n",
    "    # First, create a dictionary to store the new attributes\n",
    "    proximity_scores = {}\n",
    "    for u, v, key, data in graph.edges(data=True, keys=True):\n",
    "        # Get the distances for the two nodes connected by the edge\n",
    "        dist_u = distance_to_fraud.get(u, -1)\n",
    "        dist_v = distance_to_fraud.get(v, -1)\n",
    "    \n",
    "        # Handle disconnected components by assigning a large value\n",
    "        if dist_u == -1 or dist_v == -1:\n",
    "            proximity = -1 # Or a very large number like float('inf')\n",
    "        else:\n",
    "            # The proximity of the edge is the minimum of the two endpoint distances\n",
    "            proximity = min(dist_u, dist_v)\n",
    "\n",
    "        # Store the result, using the transaction_id as the key\n",
    "        proximity_scores[data['Transaction_Id']] = proximity\n",
    "\n",
    "\n",
    "        # Now, update the original DataFrame with the new feature\n",
    "    dataframe['proximity_to_fraud'] = dataframe['Transaction_Id'].map(proximity_scores)\n",
    "    dataframe['proximity_to_fraud'].fillna(-1, inplace=True)\n",
    "\n",
    "    # Update graph so that it has 'proximity to fraud as an edge attribute\n",
    "\n",
    "    new_graph = nx.from_pandas_edgelist(\n",
    "    dataframe,\n",
    "    source = 'Sender_Account',\n",
    "    target = 'Bene_Account',\n",
    "    edge_attr=['USD_amount', 'Label', 'timestamp', 'Transaction_Id', 'proximity_to_fraud'],\n",
    "    create_using=nx.MultiDiGraph())\n",
    "\n",
    "\n",
    "\n",
    "    ## Louvain Community Partitioner \n",
    "    partitions = community_louvain.best_partition(new_graph.to_undirected(), weight='proximity_to_fraud')\n",
    "    Louvain_community_df = pd.DataFrame.from_dict(partitions, orient='index', columns=['community_id'])\n",
    "    Louvain_community_df.index.name = 'Account'\n",
    "    #Louvain_community_df = Louvain_community_df.reset_index()\n",
    "    #Louvain_community_df['Account'] = Louvain_community_df['Account'].astype(str)\n",
    "\n",
    "\n",
    "    # Merge community information back into the transaction DataFrame\n",
    "    dataframe = pd.merge(\n",
    "        dataframe,\n",
    "        Louvain_community_df, \n",
    "        left_on='Sender_Account',\n",
    "        right_on='Account', \n",
    "        right_index=True, \n",
    "        how='left', \n",
    "        suffixes=('', '_sender')\n",
    "    )\n",
    "    dataframe = dataframe.merge(\n",
    "        Louvain_community_df, \n",
    "        left_on='Bene_Account',\n",
    "        right_on='Account', \n",
    "        right_index=True, \n",
    "        how='left', \n",
    "        suffixes=('_sender', '_beneficiary')\n",
    "    )\n",
    "\n",
    "    # Create community-based features\n",
    "    dataframe['same_community'] = (dataframe['community_id_sender'] == dataframe['community_id_beneficiary']).astype(int)\n",
    "\n",
    "    # Calculate and map the average fraud rate within each community\n",
    "    community_fraud_rate = dataframe.groupby('community_id_sender')['Label'].mean().to_dict()\n",
    "    dataframe['community_fraud_rate_sender'] = dataframe['community_id_sender'].map(community_fraud_rate)\n",
    "    dataframe['community_fraud_rate_beneficiary'] = dataframe['community_id_beneficiary'].map(community_fraud_rate)\n",
    "\n",
    "    ''''# Calculate and map the size of each community\n",
    "    community_size = Louvain_community_df.groupby('community_id')['Account'].count().to_dict()\n",
    "    dataframe['community_size_sender'] = dataframe['community_id_sender'].map(community_size)\n",
    "    dataframe['community_size_beneficiary'] = dataframe['community_id_beneficiary'].map(community_size)'''\n",
    "\n",
    "\n",
    "\n",
    "    ## Feature Engineering \n",
    "\n",
    "\n",
    "    # determines which accounts are known to have been involved in a fraudulent transaction\n",
    "    known_fraud_accounts = set(dataframe[dataframe['Label'] == 1]['Sender_Account']).union(set(dataframe[dataframe['Label'] == 1]['Bene_Account']))\n",
    "    df_known_fraud = pd.DataFrame(known_fraud_accounts, columns=['Account'])\n",
    "    df_known_fraud['is_known_fraud'] = 1\n",
    "\n",
    "    # stores number of transactions going into each account\n",
    "    df_in_degree = pd.DataFrame(new_graph.in_degree(), columns = ['Account', 'in_degree'])\n",
    "\n",
    "    # stores number of tranactions going out of each account\n",
    "    df_out_degree = pd.DataFrame(new_graph.out_degree(), columns = ['Account', 'out_degree'])\n",
    "\n",
    "    # \"computes a ranking of the nodes in the graph G based on the structure of the incoming links\" (from networkx documentation)\n",
    "    # accounts with high pagerank are highly connected. Could expose fraud rings?\n",
    "    df_pagerank = pd.DataFrame(nx.pagerank(new_graph, weight='USD_amount').items(), columns=['Account', 'pagerank'])\n",
    "    \n",
    "    \n",
    "    ## Merging the dataframes for engineered features\n",
    "    account_features = (\n",
    "    df_in_degree.merge(df_out_degree, on='Account', how='outer')\n",
    "    .merge(df_pagerank, on='Account', how='outer')\n",
    "    #.merge(df_clustering, on='Account', how='outer')\n",
    "    .merge(df_known_fraud, on='Account', how='left')\n",
    "    .fillna(0) # Fill NaN values, assuming 0 for accounts without a specific feature\n",
    "    )\n",
    "     #.merge(df_katz_centrality, on='account', how = 'outer')\n",
    "\n",
    "    # Rename columns for clarity before merging into original dataframe\n",
    "    account_features_sender = account_features.add_prefix('Sender_')\n",
    "    account_features_benefactor = account_features.add_prefix('Bene_')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ## Merging features back into the main transaction DataFrame\n",
    "    dataframe = dataframe.merge(account_features_sender, on='Sender_Account', how='left')\n",
    "    new_dataframe = dataframe.merge(account_features_benefactor, on='Bene_Account', how='left')\n",
    "\n",
    "    new_dataframe['First_Number'] = new_dataframe['USD_amount'].astype(str).str[0]\n",
    "\n",
    "\n",
    "    return new_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # Requires 'pip install pickle'\n",
    "with open('Graph_finale_train.pickle', 'rb') as f:\n",
    "        G_train = pickle.load(f)\n",
    "aug_df_train = dataframe_feature_engineerer(G_train, df_train)\n",
    "aug_df_train.to_csv('augmented_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # Requires 'pip install pickle'\n",
    "with open('Graph_finale_val.pickle', 'rb') as f:\n",
    "        G_val = pickle.load(f)\n",
    "aug_df_val = dataframe_feature_engineerer(G_val, df_val)\n",
    "aug_df_val.to_csv('augmented_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # Requires 'pip install pickle'\n",
    "with open('Graph_finale_test.pickle', 'rb') as f:\n",
    "        G_test = pickle.load(f)\n",
    "aug_df_test = dataframe_feature_engineerer(G_test, df_test)\n",
    "aug_df_test.to_csv('augmented_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
