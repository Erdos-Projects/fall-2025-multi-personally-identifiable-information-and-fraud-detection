{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import community as community_louvain\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/noimotbakare/Dropbox/Fraud_Payments/data/fraud_payment_data', sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting time into a Datetime object\n",
    "df['timestamp'] = pd.to_datetime(df['Time_step'])\n",
    "#Removing redudant columns\n",
    "df=df.drop('Time_step',axis=1)\n",
    "#df=df.drop('Sender_lob',axis=1)\n",
    "#df=df.drop('Sender_Sector',axis=1)\n",
    "#Apparently some transactions amounted to zero dollars. None of them were fraudulent, so I've removed them.\n",
    "#=====>Many credit card fraud datasets contain transactions with an amount of zero, which are actually merchants verifying that a card is active and functional.\n",
    "#While these are not fraudulent, they are part of a regular transaction pattern.\n",
    "#Removing them would eliminate a significant piece of normal customer behavior, which the model needs to understand. \n",
    "#df=df[df.USD_amount>0]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering Features that do not rely on Statistics before train test split\n",
    "\n",
    "#First Digit \n",
    "df['USD_amount'] = pd.to_numeric(df['USD_amount'], errors='coerce')\n",
    "#Extract the first digit from each amount\n",
    "df['first_digit'] = df['USD_amount'].astype(str).str.strip().str[0]\n",
    "df = df[df['first_digit'].str.isdigit()]\n",
    "df['first_digit'] = df['first_digit'].astype(int)\n",
    "\n",
    "#Last Digit \n",
    "#Extract the last digit after decimal USD_amount\n",
    "df['last_digit_after_dec'] = df['USD_amount'].astype(str).str.split('.').str[1].str[-1]\n",
    "df = df[df['last_digit_after_dec'].str.isdigit()]\n",
    "df['last_digit_after_dec'] = df['last_digit_after_dec'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take 70% of our dataset as training data, 15% as a validation set and use the remaining 15% as our test set.\n",
    "cutoff = round(0.7*len(df)) \n",
    "df_train = df.head(cutoff)\n",
    "not_train =  df.tail(len(df)-cutoff)\n",
    "cutoff2 = round(0.5*len(df_train))\n",
    "df_val = df.head(cutoff2)\n",
    "df_test = df.tail(len(not_train)-cutoff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the distribution of fraudulent/legitimate transactions are consistent across the three different sets.\n",
    "print('The distribution of fraud for the train data is:\\n', df_train['Label'].value_counts(normalize=True))\n",
    "print('The distribution of fraud for the validation set is:\\n', df_val['Label'].value_counts(normalize=True))\n",
    "print('The distribution of fraud for the test set is:\\n', df_test['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am running the graph on only the training df. \n",
    "I use an undirected graph for clustering because most clustering algorithms (like nx.clustering) are defined for undirected graphs,and treats all connections as bidirectional. It measures how well-connected a node’s neighbors are to each other (triangles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_all_graph_maps(df_train):\n",
    "    \n",
    "    #Calculates all graph features (Directed and Undirected) on the training data.\n",
    "\n",
    "    \n",
    "    #Build the Graphs (Directed and Undirected)\n",
    "    # The directed graph (G_dir) is needed for PageRank, In-Degree, Out-Degree\n",
    "    G_dir = nx.from_pandas_edgelist(\n",
    "        #training df\n",
    "        df_train,\n",
    "        source='Sender_Account',\n",
    "        target='Bene_Account',\n",
    "        edge_attr='USD_amount',\n",
    "        create_using=nx.DiGraph() \n",
    "    )\n",
    "    \n",
    "    #I need the undirected graph (G_undir) for Clustering and Louvain\n",
    "\n",
    "    G_undir = G_dir.to_undirected()\n",
    "    \n",
    "    \n",
    "    #Calculate and Store Directed Features\n",
    "    \n",
    "    #In-Degree and Out-Degree Maps \n",
    "    in_degree_map = pd.Series(dict(G_dir.in_degree()), name='in_degree')\n",
    "    out_degree_map = pd.Series(dict(G_dir.out_degree()), name='out_degree')\n",
    "    \n",
    "    #PageRank\n",
    "    pagerank_map = pd.Series(nx.pagerank(G_dir, weight='USD_amount'), name='pagerank')\n",
    "    \n",
    "    \n",
    "    #Calculate and Store Undirected Features\n",
    "    \n",
    "    #Clustering Coefficient Map (Your missing feature is now here)\n",
    "    clustering_map = pd.Series(nx.clustering(G_undir), name='clustering_coeff')\n",
    "    \n",
    "    # Louvain Community Map (MUST be on the undirected graph)\n",
    "    # This result is a dictionary: {node_id: community_id}\n",
    "    #louvain_map = community_louvain.best_partition(G_undir)\n",
    "    \n",
    "      #Unweighted Louvain\n",
    "    louvain_unweighted = community_louvain.best_partition(G_undir)\n",
    "\n",
    "    # Weighted Louvain using USD_amount\n",
    "    louvain_weighted = community_louvain.best_partition(G_undir, weight='USD_amount')\n",
    "\n",
    "    \n",
    "    #Combining all maps into one dictionary\n",
    "    return {\n",
    "        # Directed Features\n",
    "        'pagerank': pagerank_map,\n",
    "        'in_degree': in_degree_map,\n",
    "        'out_degree': out_degree_map, \n",
    "        \n",
    "        # Undirected Features\n",
    "        'clustering': clustering_map, \n",
    "        'louvain_unweighted': louvain_unweighted,\n",
    "        'louvain_weighted': louvain_weighted,        \n",
    "    }\n",
    "\n",
    "\n",
    "GLOBAL_GRAPH_MAPS = calculate_all_graph_maps(df_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Louvain Community\n",
    "I compute Louvain Community ID for weighted and unweighted. \n",
    "\n",
    "Unweighted (reach and connectivity) treats all connections equally, regardless of the number of transactions or amounts involved. Edge values are 0 or 1 ( edge exist or doesn't). Based on the count of edges Q(modularity) is calculated to measure the difference between the ACTUAL number of edges within a community and the EXPECTED number of edges if the connections were random. \n",
    "\n",
    "Such that a single transaction between teo nodes has the same influence on the community structure as 1,000 transactions. It focuses purely on reach and answers the question WHO IS CONNECTED TO WHOM?\n",
    "\n",
    "Weighted (Strength and Volume) Louvain (often preffered for transactions) The weighted approach incorporates an edge attribute which allows algorithms to prioritize stronger, more frequent, or higher value relationships. \n",
    "\n",
    "Such that stronger influences come from connections with higher weight. For example: two accounts that transfer a $1,000,000 are much more likely to be grouped into the same community than two accounts that transferred a much lower amount ($10). \n",
    "\n",
    "\n",
    "Weight are important because Unweighted might group a high-volume, low-value fraud ring with a legitimate large institution just because they share one tiny link with a random neighbor.\n",
    "\n",
    "Weighted will strongly group accounts that have large or numerous transactions, correctly identifying them as a high-flow module. This allows the algorithm to distinguish a high-value money laundering ring (strong weight) from incidental, low-value connections (weak weight).\n",
    "\n",
    "Because weighted (volume) and unweighted (reach) communities capture fundamentally different aspect of the network structure it is good practice to INCLUDE BOTH.\n",
    "\n",
    "Robustness -  If an attacker tries to hide fraudulent activity by using very low amounts (thus minimizing the Weighted influence), the Unweighted feature might still flag the connection based on the existence of the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLOBAL_GRAPH_MAPS = calculate_all_graph_maps(df_train)\n",
    "\n",
    "#Map communities back into the dataframe\n",
    "df_train['louvain_unweighted'] = df_train['Sender_Account'].map(GLOBAL_GRAPH_MAPS['louvain_unweighted']).fillna(-1)\n",
    "df_train['louvain_weighted'] = df_train['Sender_Account'].map(GLOBAL_GRAPH_MAPS['louvain_weighted']).fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_GRAPH_MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraud rates sorted ascending\n",
    "print(\"Unweighted Louvain Fraud Rates:\")\n",
    "print(df_train.groupby('louvain_unweighted')['Label'].mean().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nWeighted Louvain Fraud Rates:\")\n",
    "print(df_train.groupby('louvain_weighted')['Label'].mean().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Total communities\n",
    "5669 unweighted \n",
    "5705 weighted \n",
    "Weighted Louvain slightly splits more communities, meaning transaction volume influences clustering.\n",
    "#Note to self - values change every run.....add a random state for reproducabiltiy \n",
    "\n",
    "Communities with ≥1 fraud\n",
    "3180 (56%)\n",
    "3166 (55%)\n",
    "In both approaches, more than half of all communities contain fraud — fraud is not isolated to just a few clusters, but community detection still helps identify where it is concentrated. This suggests transaction amount influences structure, but fraud is not strictly driven by high-value connections only.\n",
    "\n",
    "Again, we should keep both features because they capture different fraud mechanisms.\n",
    "\n",
    "Over 55% of communities have fraud → fraud is not isolated.\n",
    "This implies fraud is diffused but structured.\n",
    "So the real value will come from how fraud concentrates (fraud rate) rather than just where it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique unweighted communities:\", df_train['louvain_unweighted'].nunique())\n",
    "print(\"Unique weighted communities:\", df_train['louvain_weighted'].nunique())\n",
    "\n",
    "print(\"Communities with at least one fraud (unweighted):\", (df_train.groupby('louvain_unweighted')['Label'].sum() > 0).sum())\n",
    "print(\"Communities with at least one fraud (weighted):\", (df_train.groupby('louvain_weighted')['Label'].sum() > 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Louvain looks at communities (groups of accounts that interact frequently). Then tells us how big each community is and also what fraction of accounts in that community are fraudulent.\n",
    " the X axis represents communities- lower end of the X axis small communities, while the higher end represents large communities ( known as Whale group).\n",
    "Yaxis represents fraud rate.\n",
    "So the high risk fraud rings are represented by point (0, 1.0) at 100% fraud rate, are indicative of tight knit fraud rings or mule (\"mule is an individual recruited by fraudsters to move illegally obtained funds through their own account”) account clusters.\n",
    "The majority of the communities (nomal user population) clustered around (0,0) these are small communities with zero or near zero fraud rate. They are typical users with low fraud risk.\n",
    "Then there is another oulier.\n",
    "the point with 24,000 accounts and ~1% fraud rate, these are likely the core of the legitimate userbase, lots of accounts and very little fraud.\n",
    "Basically, this tells us that fraud happens in clusters. And features that identify the clusters have an advantage over the individual-level\n",
    " features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted community stats\n",
    "community_stats_weighted = df_train.groupby('louvain_weighted').agg(\n",
    "    community_size=('Sender_Account', 'count'),\n",
    "    fraud_rate=('Label', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    community_stats_weighted['community_size'],\n",
    "    community_stats_weighted['fraud_rate'],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.title(\"Weighted Louvain: Community Size vs Fraud Rate\")\n",
    "plt.xlabel(\"Community Size (# of Accounts)\")\n",
    "plt.ylabel(\"Fraud Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unweighted louvain treats each connection between accounts equally. It only cares about connections, not how much or how often they move money. unweighted Louvain method successfully isolates small, tightly connected fraud communities. It just tells us the accounts are closely linked. This doesn’t breakdown communities as granular as including weight = USD amount. You can see that the largest community in the unweighted is 14,000 but for weighted it is 24,000.\n",
    "\n",
    "\n",
    "Both plots tell a similar story, the weighted community structure provides real financial behavior and gives a sharper segmentation of high-risk groups. It's suggested that we use both in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted community stats\n",
    "community_stats_unweighted = df_train.groupby('louvain_unweighted').agg(\n",
    "    community_size=('Sender_Account', 'count'),\n",
    "    fraud_rate=('Label', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the scatter plot using the unweighted stats\n",
    "plt.scatter(\n",
    "    community_stats_unweighted['community_size'],\n",
    "    community_stats_unweighted['fraud_rate'],\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# Update the title to reflect unweighted analysis\n",
    "plt.title(\"Unweighted Louvain: Community Size vs Fraud Rate\")\n",
    "\n",
    "# X and Y labels remain the same\n",
    "plt.xlabel(\"Community Size (# of Accounts)\")\n",
    "plt.ylabel(\"Fraud Rate\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('unweighted_louvain_community_size_vs_fraud_rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to calculate the community fraud rate, the sender community fraud rate and the Beneficiary Community Fraud Rate \n",
    " \n",
    " I am going to use Bayesian smoothing to handle communities with very few transactions. The purpose of Bayesian smoothing is to stabilize fraud rates of small communities by blending them with the gloabl average community fraud rate. In essence this prevents a tiny community with 1 fraud out of 1 transaction from having a perfect rate (100%). While a large community of say 100,000 has 1,000 fraud transactions and would get a fraud rate of 1%. So Bayesian smoothing wil pull the extreme rate of the small community towards the gloabal average to give it a more realistic estimate. Benefits - Stability, fairness, prevent overfitting, better model performance (generalizes better to unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Community Fraud Rate \n",
    "\n",
    "community_stats_weighted = df_train.groupby('louvain_weighted').agg(\n",
    "    community_size=('Sender_Account', 'count'),\n",
    "    fraud_rate=('Label', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "#Overall fraud count and total transaction count\n",
    "global_fraud_count = df_train['Label'].sum()\n",
    "global_total_tx = len(df_train)\n",
    "\n",
    "# Global average fraud rate \n",
    "global_avg_rate = global_fraud_count / global_total_tx\n",
    "\n",
    "#25th percentile of community sizes as smoothing count \n",
    "#Communities smaller than this will be heavily smoothed\n",
    "alpha = community_stats_weighted['community_size'].quantile(0.25)\n",
    "\n",
    "#Bayesian Smoothing\n",
    "community_stats_weighted['fraud_tx_count'] = community_stats_weighted['community_size'] * community_stats_weighted['fraud_rate']\n",
    "community_stats_weighted['Smoothed_Fraud_Rate'] = (\n",
    "    (alpha * global_avg_rate) + community_stats_weighted['fraud_tx_count']\n",
    ") / (\n",
    "    alpha + community_stats_weighted['community_size']\n",
    ")\n",
    "#rename \n",
    "community_stats_weighted = community_stats_weighted.rename(\n",
    "    columns={'fraud_rate': 'Raw_Fraud_Rate'}\n",
    ")\n",
    "\n",
    "community_stats_weighted\n",
    "\n",
    "print(\"\\n--- Smoothed Weighted Community Fraud Rates (Top 10) ---\")\n",
    "print(community_stats_weighted[['louvain_weighted', 'community_size', 'Raw_Fraud_Rate', 'Smoothed_Fraud_Rate']].sort_values(by='Smoothed_Fraud_Rate', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FraudFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, add_transaction_features=True,\n",
    "                  add_sector_encoding=True, \n",
    "                  add_graph_features=True\n",
    "                  ):\n",
    "        \n",
    "\n",
    "\n",
    "        #flags in the fit method\n",
    "        self.add_transaction_features = add_transaction_features\n",
    "        self.add_sector_encoding = add_sector_encoding\n",
    "        self.add_graph_features = add_graph_features\n",
    "\n",
    "        #Attributes to be learned during fit() initialized\n",
    "        self.sender_count_map = None\n",
    "        self.sender_mean_map = None\n",
    "        self.sender_std_map = None\n",
    "        self.bene_count_map = None\n",
    "        self.bene_mean_map = None\n",
    "        self.bene_std_map = None\n",
    "        self.country_fraud_ratio_map = None\n",
    "        self.sender_avg_time_map = None\n",
    "\n",
    "        #Default in case of missing values \n",
    "        self.global_country_ratio_default = 0.0\n",
    "        self.sector_target_map = None \n",
    "         #Other option is  median/mean of all time diffs\n",
    "        self.global_time_diff_default = 0.0 #\n",
    "\n",
    "        #Graph Features \n",
    "        self.pagerank_map = None\n",
    "        self.in_degree_map = None\n",
    "        self.out_degree_map = None\n",
    "        self.clustering_map = None\n",
    "        self.louvain_unweighted_map = None\n",
    "        self.louvain_weighted_map = None\n",
    "\n",
    "        self.community_fraud_rate_map = None\n",
    "        self.global_community_fraud_rate_ = 0.0\n",
    "\n",
    "    #FIT METHOD: Learns the statistics ONLY from the training data (X)\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "       \n",
    "        if y is not None and 'Label' not in X.columns:\n",
    "            X = X.copy()\n",
    "            X['Label'] = y \n",
    "        #=====================\n",
    "        #Transaction Features\n",
    "        #====================\n",
    "        if self.add_transaction_features:\n",
    "            quick = X[X['Transaction_Type'] == 'QUICK-PAYMENT']\n",
    "            \n",
    "            #Quick Payment Transaction Features\n",
    "            self.sender_count_map = quick.groupby('Sender_Account')['USD_amount'].count()\n",
    "            self.sender_mean_map = quick.groupby('Sender_Account')['USD_amount'].mean()\n",
    "            self.sender_std_map = quick.groupby('Sender_Account')['USD_amount'].std()\n",
    "\n",
    "            self.bene_count_map = quick.groupby('Bene_Account')['USD_amount'].count()\n",
    "            self.bene_mean_map = quick.groupby('Bene_Account')['USD_amount'].mean()\n",
    "            self.bene_std_map = quick.groupby('Bene_Account')['USD_amount'].std()\n",
    "\n",
    "            #Country Fraud Ratio (Target Leakage SAFE because we only FIT on X_train)\n",
    "            fraud = X[X['Label'] == 1]\n",
    "            total_count = X.groupby('Sender_Country')['Label'].count()\n",
    "            fraud_count = fraud.groupby('Sender_Country')['Label'].count().fillna(0)\n",
    "            self.country_fraud_ratio_map = fraud_count / total_count\n",
    "            #Default for new countries\n",
    "            self.global_country_ratio_default = fraud_count.sum() / total_count.sum() \n",
    "\n",
    "            #Time Difference Feature\n",
    "            X_sorted = X.sort_values(['Sender_Account', 'timestamp'])\n",
    "            # Note: diff() and dt.total_seconds() need pandas datetime objects\n",
    "            X_sorted['time_diff'] = X_sorted.groupby('Sender_Account')['timestamp'].diff().dt.total_seconds()\n",
    "            self.sender_avg_time_map = X_sorted.groupby('Sender_Account')['time_diff'].mean()\n",
    "            #median for robustness\n",
    "            self.global_time_diff_default = self.sender_avg_time_map.median() \n",
    "        #=====================\n",
    "        #Sector Feature\n",
    "        #=====================   \n",
    "         \n",
    "        if self.add_sector_encoding:\n",
    "            X['Sender_Sector'] = X['Sender_Sector'].fillna(-1) \n",
    "            fraud_rate_by_sector = X.groupby('Sender_Sector')['Label'].mean()\n",
    "            self.sector_target_map_ = fraud_rate_by_sector\n",
    "            self.global_fraud_rate_ = X['Label'].mean()  \n",
    "        #====================\n",
    "        #Graph Feature \n",
    "        #=====================\n",
    "      \n",
    "        if self.add_graph_features:\n",
    "            #Graphs (Directed and Undirected)\n",
    "            G_dir = nx.from_pandas_edgelist(\n",
    "                X, source='Sender_Account', target='Bene_Account', \n",
    "                edge_attr='USD_amount', create_using=nx.DiGraph() \n",
    "            )\n",
    "            G_undir = G_dir.to_undirected()\n",
    "            \n",
    "            #Calculate and Store Directed Features\n",
    "            self.in_degree_map = pd.Series(dict(G_dir.in_degree()), name='in_degree')\n",
    "            self.out_degree_map = pd.Series(dict(G_dir.out_degree()), name='out_degree')\n",
    "            self.pagerank_map = pd.Series(nx.pagerank(G_dir, weight='USD_amount'), name='pagerank')\n",
    "\n",
    "            #Calculate and Store Undirected Features\n",
    "            self.clustering_map = pd.Series(nx.clustering(G_undir), name='clustering_coeff')\n",
    "            self.louvain_unweighted_map = community_louvain.best_partition(G_undir)\n",
    "            self.louvain_weighted_map = community_louvain.best_partition(G_undir, weight='USD_amount')\n",
    "\n",
    "            #Calculate Louvain Community Fraud Rate\n",
    "\n",
    "            #Weighted\n",
    "            X['Sender_Community_ID'] = X['Sender_Account'].map(self.louvain_weighted_map).fillna(-1)\n",
    "\n",
    "            #Stats for Smoothing\n",
    "            community_stats = X.groupby('Sender_Community_ID').agg(\n",
    "                community_size=('Sender_Account', 'count'), \n",
    "                fraud_rate=('Label', 'mean'),\n",
    "                fraud_tx_count=('Label', 'sum') \n",
    "            ).reset_index()\n",
    "\n",
    "            #Global Metrics (The Prior)\n",
    "             #This is the global average rate\n",
    "            self.global_community_fraud_rate_ = X['Label'].mean()\n",
    "\n",
    "\n",
    "            #Smoothing Factor (alpha)\n",
    "            #Use the 25th percentile of community sizes as the smoothing strength ( I forgot why I chose this)\n",
    "            alpha = community_stats['community_size'].quantile(0.25)\n",
    "\n",
    "            #Apply Bayesian Smoothing\n",
    "            community_stats['Smoothed_Fraud_Rate'] = (\n",
    "            (alpha * self.global_community_fraud_rate_) + community_stats['fraud_tx_count']\n",
    "            ) / ( alpha + community_stats['community_size'] )\n",
    "\n",
    "            #smoothed rate map\n",
    "            self.community_fraud_rate_map = community_stats.set_index('Sender_Community_ID')['Smoothed_Fraud_Rate']\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #print(\"Fit method completed successfully.\")\n",
    "        return self\n",
    "\n",
    " \n",
    "    #=============\n",
    "    #TRANSFORM\n",
    "    #=============\n",
    "\n",
    "    #TRANSFORM METHOD: Applies the learned statistics to create the new features\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Helper function for merging and imputing (Applies to all maps)\n",
    "        def apply_map(df, map_series, on_col, new_col_name, default_val):\n",
    "            # Convert series to a DataFrame for clean merging\n",
    "            map_df = map_series.to_frame(name=new_col_name)\n",
    "            \n",
    "          \n",
    "            df = df.merge(map_df, on=on_col, how='left')\n",
    "            \n",
    "            # Impute (Fill NaNs with the global default from the training set)\n",
    "            df[new_col_name].fillna(default_val, inplace=True)\n",
    "            return df\n",
    "\n",
    "        if self.add_transaction_features:\n",
    "            # Apply all quick payment maps\n",
    "            X_transformed = apply_map(X_transformed, self.sender_count_map, 'Sender_Account', 'Sender_quick_count', 0)\n",
    "            X_transformed = apply_map(X_transformed, self.sender_mean_map, 'Sender_Account', 'Sender_quick_mean', 0)\n",
    "            X_transformed = apply_map(X_transformed, self.sender_std_map, 'Sender_Account', 'Sender_quick_std', 0)\n",
    "            \n",
    "            X_transformed = apply_map(X_transformed, self.bene_count_map, 'Bene_Account', 'Bene_quick_count', 0)\n",
    "            X_transformed = apply_map(X_transformed, self.bene_mean_map, 'Bene_Account', 'Bene_quick_mean', 0)\n",
    "            X_transformed = apply_map(X_transformed, self.bene_std_map, 'Bene_Account', 'Bene_quick_std', 0)\n",
    "            \n",
    "            #Apply Country Fraud Ratio\n",
    "            X_transformed = apply_map(X_transformed, self.country_fraud_ratio_map, 'Sender_Country', 'Country_Fraud_Ratio', self.global_country_ratio_default)\n",
    "            \n",
    "            #Apply Sender Avg Time\n",
    "            X_transformed = apply_map(X_transformed, self.sender_avg_time_map, 'Sender_Account', 'Sender_Avg_Time_Diff', self.global_time_diff_default)\n",
    "\n",
    "    \n",
    "        if self.add_sector_encoding:\n",
    "            X_transformed['Sender_Sector'] = X_transformed['Sender_Sector'].fillna(-1)\n",
    "            X_transformed['Sender_Sector_target_enc'] = X_transformed['Sender_Sector'].map(self.sector_target_map_)\n",
    "            X_transformed['Sender_Sector_target_enc'].fillna(self.global_fraud_rate_, inplace=True)\n",
    "\n",
    "\n",
    "        if self.add_graph_features:\n",
    "            def apply_graph_map_pair(df, map_series, map_name, default_val):\n",
    "                # Apply map to SENDER\n",
    "                df[f'Sender_{map_name}'] = df['Sender_Account'].map(map_series).fillna(default_val)\n",
    "                # Apply map to BENEFACTOR\n",
    "                df[f'Bene_{map_name}'] = df['Bene_Account'].map(map_series).fillna(default_val)\n",
    "                return df\n",
    "            \n",
    "            #Apply all structural features (Default usually 0 for centrality features)\n",
    "            X_transformed = apply_graph_map_pair(X_transformed, self.pagerank_map, 'pagerank', 0)\n",
    "            X_transformed = apply_graph_map_pair(X_transformed, self.in_degree_map, 'in_degree', 0)\n",
    "            X_transformed = apply_graph_map_pair(X_transformed, self.out_degree_map, 'out_degree', 0)\n",
    "            X_transformed = apply_graph_map_pair(X_transformed, self.clustering_map, 'clustering_coeff', 0)\n",
    "\n",
    "            # Apply Weighted Louvain Community IDs (Default is -1 for new accounts) (USED FOR FRAUD RATE CALCULATION)\n",
    "            X_transformed['Sender_Community_ID_W'] = X_transformed['Sender_Account'].map(self.louvain_weighted_map).fillna(-1)\n",
    "            X_transformed['Bene_Community_ID_W'] = X_transformed['Bene_Account'].map(self.louvain_weighted_map).fillna(-1)\n",
    "\n",
    "\n",
    "            #Apply Unweighted Louvain Community IDs ---\n",
    "            X_transformed['Sender_Community_ID_UW'] = X_transformed['Sender_Account'].map(self.louvain_unweighted_map).fillna(-1)\n",
    "            X_transformed['Bene_Community_ID_UW'] = X_transformed['Bene_Account'].map(self.louvain_unweighted_map).fillna(-1)\n",
    "            \n",
    "            #Apply the New Community Fraud Rate Feature!\n",
    "            X_transformed['Community_Fraud_Rate'] = (\n",
    "                X_transformed['Sender_Community_ID_W']\n",
    "                .map(self.community_fraud_rate_map)\n",
    "                .fillna(self.global_community_fraud_rate_) # Impute with global training mean\n",
    "            )\n",
    "\n",
    "        return X_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fe = FraudFeatureEngineer(\n",
    "    add_transaction_features=True,\n",
    "    add_sector_encoding=True,\n",
    "    add_graph_features=True\n",
    ")\n",
    "\n",
    "\n",
    "#Transformer learns the relationship from the training data \n",
    "fe.fit(df_train)\n",
    "#FE training set - adding colums into df_train\n",
    "df_train_transformed = fe.transform(df_train)\n",
    "#Applying same feature engineering to test set \n",
    "df_test_transformed = fe.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering For Training Set Manual Calculation to ensure correct calculations \n",
    "\n",
    "quick_payments_train = df_train[df_train['Transaction_Type'] == 'QUICK-PAYMENT']\n",
    "\n",
    "# Calculate the mean 'Amount' for 'Quick-Payments' per sender\n",
    "sender_mean_map = quick_payments_train.groupby('Sender_Account')['USD_amount'].mean()\n",
    "\n",
    "# Group the filtered data by Sender_ID and calculate the mean of the Amount. \n",
    "# This creates sender-to-mean mapping.\n",
    "sender_mean_map = sender_mean_map.to_frame(name='Sender_mean_quick_payments')\n",
    "\n",
    "# df_train = df_train.merge(\n",
    "#     sender_mean_map, \n",
    "#     on='Sender_Account', \n",
    "#     how='left'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_mean_map.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
