{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "c5581249-e59e-458b-a0b2-19520deecdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d824f7b-85a6-4c70-94ea-4e6261858390",
   "metadata": {},
   "source": [
    "                                                       Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "0806ad42-a398-4da7-a177-5e43711d9703",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_csv('fraud_payment_data', sep=',', header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "60bc5d00-7104-44dd-b848-d8219cef3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing redudant columns\n",
    "total_data=total_data.drop('Time_step',axis=1)\n",
    "total_data=total_data.drop('Sender_lob',axis=1)\n",
    "total_data=total_data.drop('Sender_Id',axis=1)\n",
    "total_data=total_data.drop('Bene_Id',axis=1)\n",
    "total_data=total_data.drop('Transaction_Id',axis=1)\n",
    "\n",
    "#Apparently some transactions amounted to zero dollars. None of them were fraudulent, so I've removed them.\n",
    "total_data=total_data[total_data.USD_amount>0]\n",
    "#The NaNs represent self-transactions. Correct for these here.\n",
    "total_data['Sender_Country']=total_data['Sender_Country'].fillna(total_data['Bene_Country'])\n",
    "total_data['Bene_Country']=total_data['Bene_Country'].fillna(total_data['Sender_Country'])\n",
    "total_data['Sender_Sector'] = total_data['Sender_Sector'].fillna(-1)\n",
    "total_data['Sender_Account']=total_data['Sender_Account'].fillna(total_data['Bene_Account'])\n",
    "total_data['Bene_Account']=total_data['Bene_Account'].fillna(total_data['Sender_Account'])\n",
    "\n",
    "total_data=total_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1654d9f-ace5-4300-be94-000339a3cda9",
   "metadata": {},
   "source": [
    "                                                      Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "97078308-4d40-4ca0-8b07-65315be56c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode Transaction Type\n",
    "Type_feature= pd.get_dummies(total_data['Transaction_Type'], drop_first=True).astype(int)\n",
    "countries=list(set(total_data['Sender_Country']).union(set(total_data['Bene_Country'])))\n",
    "sectors=list(set(total_data['Sender_Sector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "ef4fc609-813f-4b80-ac6d-629ed0bb2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to model the probability distribution of amount spent/recieved in a transaction given a historical average.\n",
    "#Returns probability of the transaction amount being greater than or equal to the one observed\n",
    "from scipy.stats import expon\n",
    "def amount_prob(mean,amount):\n",
    "  return expon.cdf(x=amount, scale=mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "d47ab865-5e31-4789-8ba9-7c02edbfd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that takes as input a node (an account), and a graph of transactions, and outputs the smallest  \n",
    "## number of transactions seperating the account from an account that had previously been involved in fraud. \n",
    "## The number of transactions is counted in the forward direction (outgoing transactions), and been restricted to at maximum length 5.\n",
    "## If greater than 5, then just return a huge value so the corresponding feature has a negligent value.\n",
    "def min_path_to_fraud(sender_node,G):\n",
    "    reach=nx.single_source_shortest_path_length(G, sender_node, cutoff=5)\n",
    "    Fraud_nodes=[node for node in reach.keys() if G.nodes[node]['Fraud_count']>0]\n",
    "    shortest_lengths = [reach[node] for node in Fraud_nodes]\n",
    "    if shortest_lengths:\n",
    "        return min(shortest_lengths)\n",
    "    else:\n",
    "        return 100000000 \n",
    "\n",
    "## An alternative measure of connectedness to the above. This measures what percentage of the nodes within 5\n",
    "## transactions of the given node are historically involved in fraud.\n",
    "def fraud_centrality(node,G):\n",
    "    reach=nx.single_source_shortest_path_length(G, node, cutoff=5).keys()\n",
    "    return len([node for node in reach if G.nodes[node]['Fraud_count']>0])/len(reach)\n",
    "\n",
    "##Returns whether a pair of sender, beneficiary has had a fraudulent transaction before\n",
    "def repeat_fraud(G,sender_node,bene_node):\n",
    "    Fraud= False\n",
    "    for transaction in G[sender_node][bene_node]:\n",
    "        if G[sender_node][bene_node][transaction]['Label']==1:\n",
    "            Fraud=True\n",
    "    return Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dd690-ec70-41fd-b5ba-6b514d6bf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initiate graph\n",
    "G = nx.MultiDiGraph()\n",
    "##Define dictionaries to keep track of fraud rates by country and sector\n",
    "country_dict=dict.fromkeys(countries, [0,0])\n",
    "sector_dict=dict.fromkeys(sectors, [0,0])\n",
    "\n",
    "##Initiate features we aim to engineer\n",
    "##bene/sender_prob is the probability explained above, Fraud_percentage_bene/sender is the percentage of fraudulent transactions among all \n",
    "##transactions (ingoing and outgoing) the sender account has been involved in, and Fraud_index_bene/sender is the distance given by the \n",
    "##min_path function above. Fraud centrality is given by the above function.  \n",
    "features_bene= pd.DataFrame(columns=['bene_prob','Fraud_percentage_bene','Fraud_index_bene',\n",
    "                                   'Fraud_centrality_bene','bene_in_deg','bene_out_deg','fraud_rate_by_country_bene'])\n",
    "features_sender= pd.DataFrame(columns=['sender_prob','Fraud_percentage_sender','Fraud_index_sender',\n",
    "                                    'Fraud_centrality_sender','sender_in_deg','sender_out_deg','fraud_rate_by_country_sender'])\n",
    "\n",
    "##Repeat fraud is given by the function above.\n",
    "features_general=pd.DataFrame(columns=['Repeat_Fraud','fraud_rate_by_sector'])\n",
    "for index, row in total_data.iterrows():\n",
    "    if index%1000==0:\n",
    "       print(index)\n",
    "    if index%10000==0:\n",
    "       total=pd.concat([features_bene,features_sender,features_general],axis=1)\n",
    "       total.to_csv('total_features', index=False)\n",
    "       print('Progress Saved!')\n",
    "        \n",
    "    ##First build general features\n",
    "    fraud_rate_by_country_sender=country_dict[row['Sender_Country']][0]\n",
    "    fraud_rate_by_country_Bene=country_dict[row['Bene_Country']][0]\n",
    "    fraud_rate_by_sector=sector_dict[row['Sender_Sector']][0]\n",
    "\n",
    "    country_dict[row['Sender_Country']][0]=(country_dict[row['Sender_Country']][0]*country_dict[row['Sender_Country']][1]+row['Label'])/(country_dict[row['Sender_Country']][1]+1)\n",
    "    country_dict[row['Sender_Country']][1]+=1\n",
    "    country_dict[row['Bene_Country']][0]=(country_dict[row['Bene_Country']][0]*country_dict[row['Bene_Country']][1]+row['Label'])/(country_dict[row['Bene_Country']][1]+1)\n",
    "    country_dict[row['Bene_Country']][1]+=1\n",
    "    sector_dict[row['Sender_Sector']][0]=(sector_dict[row['Sender_Sector']][0]*sector_dict[row['Sender_Sector']][1]+row['Label'])/(sector_dict[row['Sender_Sector']][1]+1)\n",
    "    sector_dict[row['Sender_Sector']][1]+=1\n",
    "    \n",
    "    new= not(G.has_edge(row['Sender_Account'],row['Bene_Account']))\n",
    "    repeatfraud=(not new) and repeat_fraud(G,row['Sender_Account'],row['Bene_Account'])\n",
    "    features_general.loc[index]=[repeatfraud,fraud_rate_by_sector]\n",
    "    \n",
    "    ##Build features related to sender accounts \n",
    "    check1=G.has_node(row['Sender_Account'])\n",
    "    if check1: ## If node already exists (i.e sender account involved in some transaction before)  \n",
    "      sender_in_deg=G.in_degree(row['Sender_Account'])\n",
    "      sender_out_deg=G.out_degree(row['Sender_Account'])\n",
    "      Fraud_percentage_sender=G.nodes[row['Sender_Account']]['Fraud_count']/(sender_in_deg+sender_out_deg)                                                                               \n",
    "      Fraud_centrality_sender=fraud_centrality(row['Sender_Account'],G)\n",
    "      Fraud_index_sender=1/(1+min_path_to_fraud(row['Sender_Account'],G))\n",
    "      if sender_out_deg>0: ## If node has been involved in an outgoing transaction\n",
    "        ##Engineer sender account features \n",
    "        sender_prob=amount_prob(G.nodes[row['Sender_Account']]['total_out']/sender_out_deg,row['USD_amount'])\n",
    "        features_sender.loc[index]=[sender_prob,Fraud_percentage_sender,Fraud_index_sender,\n",
    "                                    Fraud_centrality_sender,sender_in_deg,sender_out_deg,fraud_rate_by_country_sender]\n",
    "      else:\n",
    "        ##Engineer sender account features with default value for the prob feature as zero if no outgoing transaction history.\n",
    "        features_sender.loc[index]=[0,Fraud_percentage_sender,Fraud_index_sender,\n",
    "                                    Fraud_centrality_sender,sender_in_deg,0,fraud_rate_by_country_sender]\n",
    "        \n",
    "    else:##If node does not exist, put 0 as default value for features where appropriate \n",
    "      features_sender.loc[index]=[0,0,0,0,0,0,fraud_rate_by_country_sender]\n",
    "    \n",
    "    ##Repeat the same for beneficiary account\n",
    "    check2=G.has_node(row['Bene_Account'])\n",
    "    if check2:\n",
    "      Bene_in_deg=G.in_degree(row['Bene_Account'])\n",
    "      Bene_out_deg=G.out_degree(row['Bene_Account'])\n",
    "      Fraud_percentage_Bene=G.nodes[row['Bene_Account']]['Fraud_count']/(Bene_in_deg+Bene_out_deg)                                                                                 \n",
    "      Fraud_centrality_Bene=fraud_centrality(row['Bene_Account'],G)\n",
    "      Fraud_index_Bene=1/(1+min_path_to_fraud(row['Bene_Account'],G))\n",
    "      if Bene_in_deg>0: \n",
    "        Bene_prob=amount_prob(G.nodes[row['Bene_Account']]['total_in']/Bene_in_deg,row['USD_amount'])\n",
    "        features_bene.loc[index]=[Bene_prob,Fraud_percentage_Bene,Fraud_index_Bene,\n",
    "                                    Fraud_centrality_Bene,Bene_in_deg,Bene_out_deg,fraud_rate_by_country_Bene]\n",
    "      else:\n",
    "        features_bene.loc[index]=[0,Fraud_percentage_Bene,Fraud_index_Bene,\n",
    "                                    Fraud_centrality_Bene,0,Bene_out_deg,fraud_rate_by_country_Bene]\n",
    "    else:\n",
    "      features_bene.loc[index]=[0,0,0,0,0,0,fraud_rate_by_country_Bene]\n",
    "\n",
    "    check3=(row['Sender_Account']==row['Bene_Account'])#For self-transactions\n",
    "    ##Add/update edges and nodes in the graph corresponding to the transaction\n",
    "    if check1:  \n",
    "      G.nodes[row['Sender_Account']]['total_out']+=row['USD_amount']\n",
    "      G.nodes[row['Sender_Account']]['Fraud_count']+=row['Label']\n",
    "    else:\n",
    "      G.add_node(row['Sender_Account'], total_out=row['USD_amount'], total_in=0, Fraud_count=row['Label'])\n",
    "    if check2 or check3:  \n",
    "      G.nodes[row['Bene_Account']]['total_in']+=row['USD_amount']\n",
    "      G.nodes[row['Bene_Account']]['Fraud_count']+=row['Label']\n",
    "    else:\n",
    "      G.add_node(row['Bene_Account'], total_in=row['USD_amount'], total_out=0, Fraud_count=row['Label'])\n",
    "    \n",
    "    G.add_edge(row['Sender_Account'], row['Bene_Account'],Label=row['Label'])\n",
    "\n",
    "total=pd.concat([features_bene,features_sender,features_general,Type_feature],axis=1)\n",
    "total.to_csv('total_features', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb237e6-2698-4640-8857-c92400f7ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save graph as a .pickle file\n",
    "with open('Graph_total.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f497ffe-8264-453a-bf24-cb4ecec03bcf",
   "metadata": {},
   "source": [
    "                                                Training and Calibration of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "e222fca7-0734-4cdc-82eb-3751444272fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optimizing XGBoost\n",
    "xgb_search = GridSearchCV(XGBClassifier(random_state = 831, scale_pos_weight = 49 ),\n",
    "                       param_grid= {'learning_rate':[0.01, 0.1, 1,],\n",
    "                       'n_estimators': [100, 300, 500, 700],\n",
    "                       'max_depth': [10,30,50,70,90,100]},\n",
    "                                    scoring = 'f1',\n",
    "                                    cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "1df6a69f-1434-40df-8f42-6232abf4e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features=pd.read_csv('total_features', sep=',', header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb72d0-0af1-44cc-ab76-af9c7a3c1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "32fed0f8-7c17-4a91-b066-392fa5cc3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split for train, validate and test data\n",
    "train_features=final_features[0:1000000]\n",
    "validate_features=final_features[1000000:1250000]\n",
    "test_features=final_features[1250000:-1]\n",
    "\n",
    "y_train=total_data['Label'][0:1000000]\n",
    "y_validate=total_data['Label'][1000000:1250000]\n",
    "y_test=total_data['Label'][1250000:-1]\n",
    "\n",
    "##Scaler for eventual Logistic regression\n",
    "X=StandardScaler().fit_transform(train_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175939f-7653-4a28-9c28-2b51114e7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Looking at the correlation matrix for our features in our training data\n",
    "corr_matrix = pd.concat([train_features,total_data[:1000000]['Label']],axis=1).corr()\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9354c-932c-442e-be4b-a27e39d63fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_search.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270da4c-6300-4f9d-a9a4-c5ebb4b1d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_search.best_params_)\n",
    "print(xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1f687-cbfa-4165-be8f-8d22844a0c4c",
   "metadata": {},
   "source": [
    "                                                Results and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c982a09-d70a-46c2-8721-3fafec094c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model testing \n",
    "log_reg = LogisticRegression(class_weight='balanced', penalty=None)\n",
    "xgb=XGBClassifier(learning_rate= 0.01, max_depth= 70, n_estimators= 700, scale_pos_weight = 50)\n",
    "\n",
    "## fit the model\n",
    "xgb.fit(X,y_train)\n",
    "log_reg.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5c693-f299-4892-bb05-5394314879c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test\n",
    "cutoff = 0.85\n",
    "## store the predicted probabilities\n",
    "y_prob = log_reg.predict_proba(StandardScaler().fit_transform(test_features.values))[:,1]\n",
    "## assign the value based on the cutoff\n",
    "y_pred = 1*(y_prob >= cutoff)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(f'Confusion Matrix for {log_reg.__class__.__name__}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cutoff = 0.5\n",
    "## store the predicted probabilities\n",
    "y_prob = xgb.predict_proba(StandardScaler().fit_transform(test_features.values))[:,1]\n",
    "## assign the value based on the cutoff\n",
    "y_pred = 1*(y_prob >= cutoff)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(f'Confusion Matrix for {xgb.__class__.__name__}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e2e28-3917-432d-b0c2-bfb66eb32448",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate Recall-Lift graph for logistic regression (balancing for the class imbalance). Variable recall is obtained by varying the cutoff.\n",
    "lift_scores=np.zeros(100)\n",
    "recall_scores=np.zeros(100)\n",
    "log_reg = LogisticRegression(class_weight='balanced', penalty=None)\n",
    "log_reg.fit(X,y_train)\n",
    "y_prob = log_reg.predict_proba(StandardScaler().fit_transform(test_features.values))[:,1]\n",
    "for i in range(100):\n",
    "    y_pred = 1*(y_prob >= i/100)    \n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    ppr=(tp+fp)/(tn+fp+fn+tp)\n",
    "    lift=recall/ppr\n",
    "    recall_scores[i]=recall\n",
    "    lift_scores[i]=lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20810649-aaad-4f09-aa07-7fa0215e0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate Recall-Lift graph for optimized XGBoost models with different cutoffs (balancing for the class imbalance). \n",
    "##Variable recall is obtained by varying the weight of the class.\n",
    "for j in range(5): \n",
    "  lift_scores2=np.zeros(20)\n",
    "  recall_scores2=np.zeros(20)\n",
    "  for i in range(20):\n",
    "    xgb=XGBClassifier(learning_rate= 0.01, max_depth= 70, n_estimators= 700, scale_pos_weight = 4**i)\n",
    "    xgb.fit(X,y_train)\n",
    "    y_prob=xgb.predict_proba(StandardScaler().fit_transform(test_features.values))[:,1]\n",
    "    y_pred = 1*(y_prob >= (j+1)/10)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    ppr=(tp+fp)/(tn+fp+fn+tp)\n",
    "    lift=recall/ppr\n",
    "    recall_scores2[i]=recall\n",
    "    lift_scores2[i]=lift\n",
    "    print(i)\n",
    "  plt.plot(recall_scores2, lift_scores2,label=('XGBoost with threshold='+str((j+1)/10))) \n",
    "plt.plot(recall_scores, lift_scores,label='Logistic Regression')\n",
    "plt.plot(recall_scores, np.ones(len(recall_scores)),label='Baseline')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Lift\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:erdos_ds_environment]",
   "language": "python",
   "name": "conda-env-erdos_ds_environment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
